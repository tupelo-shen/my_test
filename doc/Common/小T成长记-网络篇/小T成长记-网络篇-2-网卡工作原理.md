[TOC]

## 背景

> 时间：2021-01-31 20:00
> 
> 地点：XXX公司技术研发中心
> 
> 人物：小T（全名-Tupelo Shen），一个满脑子奇思妙想的初级软件工程师，总是喜欢打破砂锅问到底。
> 
> 老鸟（外号-隔壁老王-W），公司的高级研究员，拥有丰富的开发经验，貌似无所不知。

勤奋好学的菜鸟小T，通过查阅资料，已经对于网络的发展、作用和历史意义有了较深的理解。但是，不仅又陷入了硬件的泥沼：计算机系统中，到底谁在负责网络数据的收发？它的工作原理是什么？操作系统如何操作这些底层硬件？带着这些疑问，敲开了老鸟老王的办公室门。

## 网卡

小T：

> 师傅，我又有地方不明白了，能向您请教一下吗？

老王：

> 有哪里不明白了啊？

小T：

> 上次您给我解释了网络的概念之后，我回去查阅了OSI模型，知道了七层模型。我给您画一下，您看看对不对：

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/network_2_1_OSI.png">

看着小T，熟练地画出OSI模型各层次之间的关系和通信时的数据流向。老王满意地点了点头。

小T：

> 师傅，对于网络模型以及大概的数据流向我已经理解了。但还有一点不明白，数据链路层和物理层是不是都是硬件完成的，那它们又分别对应着什么呢？它们分别是如何工作的呢？软件又是如何和它们交互的呢？

老王：

> 看来你回去没少下功夫啊，查了不少资料啊。那对于你的疑问咱们一一解答：
> 
> 首先，数据链路层和物理层确实都是由硬件完成的。其中，数据链路层的芯片，我们一般称之为MAC控制器，物理层的芯片我们称之为PHY芯片。当然，也有厂家把MAC和PHY的功能都做到一个芯片上，比如`Intel 82559`网卡和`3COM 3C905`网卡。但是，不管是分离还是集成，MAC和PHY都是独立的机制。这一点明白了吗？

小T：

> 概念是明白了，但是MAC和PHY它们分别的作用是什么呢？

老王：

> 首先，MAC层的功能，故名思意，其主要实现MAC子层，也就是介质访问控制；以及LLC子层，它是逻辑链路控制。通俗的讲，MAC子层就是告诉对方，自己的MAC地址以及对方的MAC，这样对方才能直到这个数据的来龙去脉。而逻辑链路控制就是，有时候，MAC并不知道对方的MAC地址，需要通过一种称之为`ARP`协议的手段，发出广播数据，查看数据包中IP对应的MAC地址是谁。当然了，还包括CRC校验处理，以及一些其它的流控帧。最后，MAC芯片或者网卡一般挂载PCI总线上，还需要处理与PCI的数据交互。为了实现上述目的，需要一些控制寄存器和数据寄存器，而这些寄存器位于MAC芯片上，并由其完成各种状态的转换。
> 
> 那接下来就是PHY芯片了。从MAC层出来的数据，已经不需要数据帧的概念了，就是单纯的数字0和1。所以，PHY芯片的作用就是把这些数字信号安全有效地转换成电信号，发送出去。

小T：

> 师傅，好像明白了一些。那我重新整理一遍流程

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/net_hardware_cpu_access_io_dma.png">

## 2 CPU访问网卡的方式

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/net_hardware_cpu_access_netcard_dma.png">

## 3 网络收包原理

网络收包大致分为三种情况：

* `poll`方式

    常见于小型嵌入式设备的裸机程序。通常是一个while之类的大循环，定时查询数据包的接收情况实现。应用场景是对于网络通信的吞吐量的需求不那么迫切的情况下。

* `中断`方式

    而中断的方式是：MAC每收到一个以太网数据包就会产生一个接收中断给CPU，继而交给中断处理程序进行数据包的处理。

* `NAPI`方式

    `NAPI`采用`中断+轮询`的方式。MAC收到一个数据包后产生中断，进入中断处理程序。但是，在处理数据包之前，会先关闭接收中断。然后，一直接收数据，直到收够了N个包（Linux使用netdev_max_backlog参数表示N，默认是300）或者收完MAC上所有包之后，再打开接收中断，退出中断处理程序。

> 轮询的方式，不会增加CPU的负荷，但是实时性太差。如果网络数据量一旦增大，很容易丢失数据。而中断的方式极大地提高了数据的响应实时性。但是，也引入了新的问题。如果网络流量很大的时候，频繁地切换中断上下文，尤其是带有操作系统的情况下，极大地增加了CPU的额外负荷。而NAPI则是博众家之所长，将网络性能最大化的最佳方案。

> `NAPI`是不是很难理解。其实，没有什么特殊的意义，就是`New API`的意思。也许是当时的作者词穷了吧。

## 4 NAPI相关数据结构

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/net_hardware_NAPI_data_struct.png">

> 每个网络设备（上图中，有两个网络设备：MAC控制器和PCI网卡）都有自己描述设备的数据结构`net_device`，这个结构中有个成员是`napi_struct`，专门用来保存NAPI相关的数据。每当收到数据包时，网络设备驱动就会把自己的`napi_struct`挂到CPU私有变量上。当进入软中断时，`net_rx_action()`会遍历CPU私有变量的`poll_list`列表，找到并执行对应的`napi_struct`结构中的`poll`回调函数，将数据包从驱动传递给网络协议栈的接口。

## 5 内核启动时的网络初始化

内核启动时，需要完成网络相关的全局数据结构，并绑定处理网络相关的软中断的回调函数。

网络初始化的调用过程为

```c
start_kernel() 
    --> rest_init() 
        --> do_basic_setup()
            --> do_initcall()
                --> net_dev_init()
```

那么，我们重点关注的是`net_dev_init`函数

```c
__init net_dev_init()
{
    // 每个CPU都有一个CPU私有变量
    // _get_cpu_var(softnet_data)
    // poll_list很重要，软中断中需要遍历它
    for_each_possible_cpu(i) {
        struct softnet_data *queue;
        queue = &per_cpu(softnet_data, i);
        skb_queue_head_init(&queue->input_pkt_queue);
        queue->completion_queue = NULL;
        // 初始化poll_list列表的链表结构
        INIT_LIST_HEAD(&queue->poll_list);
        queue->backlog.poll = process_backlog;
        queue->backlog.weight = weight_p;
    }
    // 绑定软中断与网络发送处理函数
    open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL);
    // 绑定软中断与网络接收处理函数
    open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL);
}
```

## 6 加载网络设备驱动程序

加载网络设备驱动的过程，其实就是为每个网络设备创建对应的数据结构`net_device`，并让内核知道其存在的过程。我们知道，不管是CPU继承的MAC控制器还是外挂的PCI网卡一般都通过PCI总线与CPU相连。
所以，我们需要按照PCI驱动的数据结构定义一个数据结构体，在此，我们命名为`xxx_driver`。但是，现在新版本内核一般都使用`platform`虚拟的总线设备驱动完成硬件设备驱动的处理。

为了描述简单，我们简化掉对于我们理解网络驱动加载不太重要的内容：

```c
static struct platform_driver xxx_driver = {
    ...
    .probe    = xxx_probe,
    .remove   = xxx_remove,
    // 下面3个成员一般用于电源管理
    .driver = {
        .name = "设备名称字符串",
    },
    ...
};
```

PCI总线设备驱动一旦探测到设备节点存在网络设备，会调用这个probe函数，完成对应网络设备的驱动程序注册。我们来看一下这个函数的实现：

```c
int xxx_probe(struct platform_device *pdev)
{
    /* 创建net_device数据结构 */
    dev = alloc_etherdev(sizeof (*priv)); 

    dev->open = xxx_enet_open;
    dev->hard_start_xmit = xxx_start_xmit;
    dev->tx_timeout = xxx_timeout;
    dev->watchdog_timeo = TX_TIMEOUT;
#ifdef CONFIG_XXX_NAPI
    /* 软中断中会调用poll回调函数*/
    netif_napi_add(dev, &priv->napi,xxx_poll,XXX_DEV_WEIGHT); 
#endif
#ifdef CONFIG_NET_POLL_CONTROLLER
    dev->poll_controller = xxx_netpoll;
#endif
    dev->stop = xxx_close;
    dev->change_mtu = xxx_change_mtu;
    dev->mtu = 1500;
    dev->set_multicast_list = xxx_set_multi;
    dev->set_mac_address = xxx_set_mac_address;
    dev->ethtool_ops = &xxx_ethtool_ops;
}
```

## 7 启动网络设备

用户态程序调用ifconfig等工具命令时，发生的调用过程如下图所示：

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/net_hardware_net_system_call_process.png">

> 此处的`dev_open`回调函数就是前面注册的`xxx_enet_open`函数。

## 8 网络设备的open操作

那么，网络设备的open回调函数到底都做了什么呢？

分配收发缓冲区描述符，配置中断ISR（包括rx、tx、err），对于MAC控制器来说，

## 9 数据链路层接收数据包的过程

<img src="https://raw.githubusercontent.com/tupelo-shen/my_test/master/doc/Industrial_field_bus/LWIP/images/net_hardware_data_link_layer_process.png">

> 系统启动时，网卡控制器（NIC）进行初始化。向内核的内存地址空间申请内存给`Ring Buffer`。`Ring Buffer`中的每个元素（数据包描述符）都指向一个`sk_buff`。准备就绪，等待数据的传送。
> 
> 上图描述了数据链路层接收数据包，到传递给网络协议栈的过程：
> 
> ①. 网卡控制器收到数据，DMA搬运到`Ring Buffer`对应的一个`sk_buff`中，产生Rx中断。
> 
> ②. 产生Rx中断后，IRQ传递给CPU，触发相应的中断处理程序。
> 
> ③. 中断处理程序中：如果选择`NAPI`方式，则关闭中断，连续接收数据包。满足条件后，触发软中断SoftIRQ，并将相应的网络设备数据结构挂载到`NAPI`的poll列表中，等待软中断处理；如果选择非`NAPI`方式，

