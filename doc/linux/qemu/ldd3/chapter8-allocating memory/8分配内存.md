* [8.1 kmalloc](#8.1)
    * [8.1.1 参数-flags](#8.1.1)
    * [8.1.2 参数-size](#8.1.2)
* [8.2 后备高速缓存](#8.2)
* [8.3 get_free_page and Friends](#8.3)
* [8.4 Per-CPU Variables](#8.4)
* [8.5 Obtaining Large Buffers](#8.5)

***

到目前为止，我们已经使用过kmalloc和kfree两个函数分配和释放内存。但是，Linux内核还提供了丰富的内存分配函数。本章，我们将学习在驱动程序中使用其它内存方法，以及如何去优化系统内存资源。我们不会关注不同体系架构管理内存的方法的不同。驱动模块不会涉及分段，分页等，因为内核为驱动程序提供了统一的内存分配函数接口。另外，也不会讨论内存管理实现的细节，推迟到第15章。

***
<h2 id="8.1">8.1 kmalloc </h2>

`kmalloc`与`malloc`有一定的相似性。该函数速度非常快（除非它阻塞），它不会清除获取的内存；所以获取内存后，需要用户手动清除。分配的内存是一段连续的物理内存。

<h3 id="8.1.1">8.1.1 参数-flags </h3>
***

`kmalloc` 的原型是：

    #include <linux/slab.h>
    void *kmalloc(size_t size, int flags);

`size`是分配内存块的大小。`flags`是分配标志，控制`kmalloc`的几种不同行为。

给运行在内核空间的进程分配内存（内部最终调用`__get_free_pages`， 这也是`GFP_`前缀的由来）。换句话说，调用最终执行系统调用。使用`GFP_KERNEL`意味着，当在低内存的情况下调用时，`kmalloc`函数可以使当前进程休眠，等待有足够的内存分配。因此，使用`GFP_KERNEL`分配内存的函数必须是可重入的，且不能在原子上下文中运行。当前进程休眠时，内核会采取恰当的手段查找空闲内存，方法就是将缓冲区刷新到磁盘或从用户空间交换内存。

有时候，使用`GFP_KERNEL`标志不合适，比如在中断服务程序，`tasklet`，和内核定时器等中发生内存分配时，我们是不希望进程休眠的。所以，驱动程序可以使用`GFP_ATOMIC`代替。内核经常保留一些空间内存页以满足原子分配。使用`GFP_ATOMIC`时，`kmalloc`甚至可以使用最后一个空闲内存页。但是如果一个空闲内存页也没有，则分配失败。

虽然`GFP_KERNEL`和`GFP_ATOMIC`能够满足大部分设备驱动程序的需求，但除此之外，还有一些其它的标志。它们都定义在`<linux/gfp.h>`头文件中。有一些标志，使用双下划綫作为前缀，比如，`__GFP_DMA`。

* GFP_ATOMIC

    为中断服务程序或其它进程之外的代码分配内存。不会休眠。

* GFP_KERNEL

    内核内存的正常分配函数。可以休眠。

* GFP_USER

    为用户空间页面分配内存。可以休眠。

* GFP_HIGHUSER

    同`GFP_USER`，但是从高内存分配。

* GFP_NOIO
* GFP_NOFS

    这两个标志很像`GFP_KERNEL`，但是对于内核可以做什么作了限制。`GFP_NOFS`不允许执行文件系统调用，而`GFP_NOIO`不允许任何I/O初始化。它们主要用在文件系统和虚拟内存代码中，允许休眠，但是递归文件系统调用是个坏主意。

上面列出的分配标志可以和下面的标志进行`或`操作，组合使用，以改变分配方式：

* __GFP_DMA

    请求在具有DMA功能的内存区域进行分配。具体依赖于平台。

* __GFP_HIGHMEM

    从高内存区域分配内存。关于高内存看下面的介绍。

* __GFP_COLD

    通常情况下，内存分配器尽可能分配“高速缓存”。相反，该标志就是请求一段时间内没有使用的内存页。对于DMA读请求非常有用，但是对高速缓存就没有啥用了。

* __GFP_NOWARN

    阻止请求失败时内核发出的警告信息。几乎不用。

* __GFP_HIGH

    高优先级请求，即使内核为紧急事件准备的备用内存它也可以使用。

* __GFP_REPEAT

    重复尝试分配，但仍然有可能会失败。

* __GFP_NOFAIL

    告诉内核分配内存不能失败，不建议使用。

* __GFP_NORETRY

    告诉分配器，如果请求内存不可用立即放弃。

组合使用方法举例：

1. 用于DMA的内存，可以休眠：   GFP_DMA | GFP_KERNEL

2. 用于DMA的内存，不可以休眠：  GFP_DMA |GFP_ATOMIC


####内存区域
***

`__GFP_DMA`和`__GFP_HIGHMEM`是依赖于平台的，尽管在所有的平台上都可以指定。

Linux 内核至少应该可以访问三种内存区域： DMA内存， 普通内存， 高内存。设置不同的标志位，可以访问不同的内存区域。

DMA内存是外设可以执行DMA访问的优先内存地址。在x86系统上，DMA内存区域使用RAM起始的16MB空间，`legacy ISA`设备可以执行DMA访问；PCI设备则没有这个限制。

高内存是在32位系统上访问大容量内存的一种机制。如果没有建立特殊的内存映射，内核无法直接访问高地址内存。

三种内存区域的比较：

| 区 | 描述 | 物理内存 |
| ------ | ------ | ------ |
| DMA内存 | 可以DMA访问的内存 | <16MB |
| 普通内存 | 正常可寻址的内存 | 16~896MB |
| 高内存 | 动态映射的内存 | >896MB |

每当要分配内存时，内核先确定指定区域内是否有可用的内存页。如果`__GFP_DMA`被指定，仅在DMA区域内搜索：如果没有地址可用，则分配失败。如果没有指定分配标志，则普通内存和DMA内存都会被搜索。如果设置了`__GFP_HIGHMEM`，则上面提到的三个区域都会被搜索。（但是，请注意，`kmalloc`不能分配高内存。

<h3 id="8.1.2">8.1.2 参数-size </h3>
***

内核管理的系统物理内存必须是页面化的块。结果就是，`kmalloc`与典型的用户空间的`malloc`实现相当不同。简单的面向堆分配技术在内存页边界处会很难处理。因此，内核使用了特殊的面向页分配技术，从而更好地使用系统的RAM。

Linux 通过创建一组固定大小的内存池对象来处理内存分配。从内存池中选取一整个内存块返回给请求者。内存管理的原理相当复杂，对于设备驱动程序开发来说，不必关心其实现细节。

但是，对于驱动开发者来说，最应该记住的就是，内核只能分配某些预定义的固定大小的字节数组。如果动态请求内存的话，可能得到的比请求的多，最大可以是2倍。另外，请记住，`kmalloc`能够处理的最小分配单元就是32或64字节，依赖于系统体系架构使用的页大小。

`kmalloc`能够分配的内存块的大小是有上限的。这个限制根据体系架构和内核配置选项而不同。如果想要代码具有更好的可移植性，最好不要分配大于128KB的内存空间。如果需要几KB以上的空间，有比`kmalloc`更好的方法，后面会有讨论。

***
<h2 id="8.2">8.2 后备高速缓存 </h2>

既然，驱动程序需要反反复复地请求分配相同大小的内存对象。而内核已经维护了一组相同大小的内存池对象，那为什么不给这些大量的对象建立特别的内存池呢？事实上，内核有一个创立这类内存池的工具，称为后备高速缓存。设备驱动程序通常不会使用它，但是Linux 2.6以后版本的内核中，USB和SCSI使用了高速缓存。

在linux内核中，cache管理器又称为“slab分配器”。创建高速缓存的函数原型如下所示，声明位于`<linux/slab.h>`文件中。

    kmem_cache_t *kmem_cache_create(const char *name, size_t size, size_t offset, unsigned long flags,
        void (*constructor)(void *, kmem_cache_t *, unsigned long flags),
        void (*destructor)(void *, kmem_cache_t *, unsigned long flags));

该函数返回一块类型为`kmem_cache_t`的高速缓存，在其中包含许多相同大小的内存块，大小由参数`size`指定。`name`，就是作为追踪问题时的信息。通常被命名为要保存的结构体的类型。缓存只是包含了`name`的一个指针，并没有复制它的内容，所以，必须给该函数传递一个`static`存储空间中的字符串的指针。该字符串不能包含空格。`offset`是页内的第一个对象的偏移量；最好使用0作为默认值。`flags`控制分配方式，是以下标志的位掩码。

* SLAB_NO_REAP

    设置此标志可防止在系统查找内存时缓存减少。 设置此标志通常是个坏主意; 重要的是避免不必要地限制内存分配器的操作自由。

* SLAB_HWCACHE_ALIGN

    设置这个标志，每个数据对象与高速缓存行对齐；真实的对齐方式还依赖于主机平台的缓存布局。在SMP计算机上，对于经常访问的内容，此选项是一个不错的选择。但是，为了满足缓存行对齐，实施填充也浪费了大量的内存。

* SLAB_CACHE_DMA

    要求每个数据对象被分配到DMA内存中。

参数`constructor`和`destructor`是可选项，前者可以用来初始化分配的对象；后者可以用来在释放内存到系统中时清空内存。

虽然，参数`constructor`和`destructor`很有用，但是也有些限制。当为一组对象分配内存时就会调用`constructor`；因为该内存块存储了几个对象，`constructor`可能会被调用好几次。不能假设`constructor`再分配对象时会立即起作用。同理，`destructor`也是如此。`constructor`和`destructor`是否能休眠，取决于是否传递了`SLAB_CTOR_ATOMIC`标志（`CTOR`是`constructor`的缩写）。

为了方便，`constructor`和`destructor`可以使用相同的函数；当调用的是`constructor`时，`slab`分配器总是传递`SLAB_CTOR_CONSTRUCTOR`标志。

一旦缓存建立，就可以调用`kmem_cache_alloc`为对象分配高速缓存空间了:

    void *kmem_cache_alloc(kmem_cache_t *cache, int flags);

在这儿，`cache`是前面创建的高速缓存；`flags`和传递给`kmalloc`的一样 and are consulted if kmem_cache_alloc needs to go out and allocate more memory itself.

释放对象，使用`kmem_cache_free`:

    void kmem_cache_free(kmem_cache_t *cache, const void *obj);

When driver code is finished with the cache, typically when the module is unloaded,
it should free its cache as follows:

    int kmem_cache_destroy(kmem_cache_t *cache);

The destroy operation succeeds only if all objects allocated from the cache have
been returned to it. Therefore, a module should check the return status from
kmem_cache_destroy; a failure indicates some sort of memory leak within the module (since some of the objects have been dropped).

One side benefit to using lookaside caches is that the kernel maintains statistics on cache usage. These statistics may be obtained from /proc/slabinfo.

***
<h3 id="8.2.1">8.2.1 一个基于Slab缓存的scull：scullc </h3>
***

Time for an example. scullc is a cut-down version of the scull module that implements only the bare device—the persistent memory region. Unlike scull, which uses kmalloc, scullc uses memory caches. The size of the quantum can be modified at compile time and at load time, but not at runtime—that would require creating a new memory cache, and we didn’t want to deal with these unneeded details.

scullc is a complete example that can be used to try out the slab allocator. It differs from scull only in a few lines of code. First, we must declare our own slab cache:

    /* declare one cache pointer: use it for all devices */
    kmem_cache_t *scullc_cache;

The creation of the slab cache is handled (at module load time) in this way:

    /* scullc_init: create a cache for our quanta */
    scullc_cache = kmem_cache_create("scullc", scullc_quantum, 0,
            SLAB_HWCACHE_ALIGN, NULL, NULL); /* no ctor/dtor */
    if (!scullc_cache) {
        scullc_cleanup( );
        return -ENOMEM;
    }

This is how it allocates memory quanta:

    /* Allocate a quantum using the memory cache */
    if (!dptr->data[s_pos]) {
        dptr->data[s_pos] = kmem_cache_alloc(scullc_cache, GFP_KERNEL);
        if (!dptr->data[s_pos])
            goto nomem;
        memset(dptr->data[s_pos], 0, scullc_quantum);
    }

And these lines release memory:

    for (i = 0; i < qset; i++)
    if (dptr->data[i])
        kmem_cache_free(scullc_cache, dptr->data[i]);

Finally, at module unload time, we have to return the cache to the system:

    /* scullc_cleanup: release the cache of our quanta */
    if (scullc_cache)
        kmem_cache_destroy(scullc_cache);

The main differences in passing from scull to scullc are a slight speed improvement and better memory use. Since quanta are allocated from a pool of memory fragments of exactly the right size, their placement in memory is as dense as possible, as opposed to scull quanta, which bring in an unpredictable memory fragmentation.

***
<h3 id="8.2.2">8.2.2 内存池 </h3>
***

There are places in the kernel where memory allocations cannot be allowed to fail. As a way of guaranteeing allocations in those situations, the kernel developers created an abstraction known as a memory pool (or “mempool”). A memory pool is really just a form of a lookaside cache that tries to always keep a list of free memory around for use in emergencies.

A memory pool has a type of mempool_t (defined in <linux/mempool.h>); you can create one with mempool_create:

    mempool_t *mempool_create(int min_nr,
                            mempool_alloc_t *alloc_fn,
                            mempool_free_t *free_fn,
                            void *pool_data);

The min_nr argument is the minimum number of allocated objects that the pool should always keep around. The actual allocation and freeing of objects is handled by alloc_fn and free_fn, which have these prototypes:

    typedef void *(mempool_alloc_t)(int gfp_mask, void *pool_data);
    typedef void (mempool_free_t)(void *element, void *pool_data);

The final parameter to mempool_create (pool_data) is passed to alloc_fn and free_fn.

If need be, you can write special-purpose functions to handle memory allocations for mempools. Usually, however, you just want to let the kernel slab allocator handle that task for you. There are two functions (mempool_alloc_slab and mempool_free_slab) that perform the impedance matching between the memory pool allocation prototypes and kmem_cache_alloc and kmem_cache_free. Thus, code that sets up memory pools often looks like the following:

    cache = kmem_cache_create(. . .);
    pool = mempool_create(MY_POOL_MINIMUM,
                        mempool_alloc_slab, mempool_free_slab,
                        cache);

Once the pool has been created, objects can be allocated and freed with:

    void *mempool_alloc(mempool_t *pool, int gfp_mask);
    void mempool_free(void *element, mempool_t *pool);

When the mempool is created, the allocation function will be called enough times to create a pool of preallocated objects. Thereafter, calls to mempool_alloc attempt to acquire additional objects from the allocation function; should that allocation fail, one of the preallocated objects (if any remain) is returned. When an object is freed with mempool_free, it is kept in the pool if the number of preallocated objects is currently below the minimum; otherwise, it is to be returned to the system.

A mempool can be resized with:

    int mempool_resize(mempool_t *pool, int new_min_nr, int gfp_mask);

This call, if successful, resizes the pool to have at least new_min_nr objects.

If you no longer need a memory pool, return it to the system with:

    void mempool_destroy(mempool_t *pool);

You must return all allocated objects before destroying the mempool, or a kernel oops results.

If you are considering using a mempool in your driver, please keep one thing in mind: mempools allocate a chunk of memory that sits in a list, idle and unavailable for any real use. It is easy to consume a great deal of memory with mempools. In almost every case, the preferred alternative is to do without the mempool and simply deal with the possibility of allocation failures instead. If there is any way for your driver to respond to an allocation failure in a way that does not endanger the integrity of the system, do things that way. Use of mempools in driver code should be rare.


***
<h2 id="8.3">8.3 get_free_page and Friends </h2>
***

If a module needs to allocate big chunks of memory, it is usually better to use a pageoriented technique. Requesting whole pages also has other advantages, which are introduced in Chapter 15.

To allocate pages, the following functions are available:

* get_zeroed_page(unsigned int flags);

    Returns a pointer to a new page and fills the page with zeros.

* __get_free_page(unsigned int flags);

    Similar to get_zeroed_page, but doesn’t clear the page.

* __get_free_pages(unsigned int flags, unsigned int order);

    Allocates and returns a pointer to the first byte of a memory area that is potentially several (physically contiguous) pages long but doesn’t zero the area.

The flags argument works in the same way as with kmalloc; usually either GFP_KERNEL or GFP_ATOMIC is used, perhaps with the addition of the __GFP_DMA flag (for memory that can be used for ISA direct-memory-access operations) or __GFP_HIGHMEM when high memory can be used.* order is the base-two logarithm of the number of pages you are requesting or freeing (i.e., log2N). For example, order is 0 if you want one page and 3 if you request eight pages. If order is too big (no contiguous area of that size is available), the page allocation fails. The get_order function, which takes an integer argument, can be used to extract the order from a size (that must be a power of two) for the hosting platform. The maximum allowed value for order is 10 or 11 (corresponding to 1024 or 2048 pages), depending on the architecture. The chances of an order-10 allocation succeeding on anything other than a freshly booted system with a lot of memory are small, however.

If you are curious, /proc/buddyinfo tells you how many blocks of each order are available for each memory zone on the system.

When a program is done with the pages, it can free them with one of the following functions. The first function is a macro that falls back on the second:

    void free_page(unsigned long addr);
    void free_pages(unsigned long addr, unsigned long order);

If you try to free a different number of pages from what you allocated, the memory map becomes corrupted, and the system gets in trouble at a later time.

It’s worth stressing that __get_free_pages and the other functions can be called at any time, subject to the same rules we saw for kmalloc. The functions can fail to allocate memory in certain circumstances, particularly when GFP_ATOMIC is used. Therefore, the program calling these allocation functions must be prepared to handle an allocation failure.

Although kmalloc(GFP_KERNEL) sometimes fails when there is no available memory, the kernel does its best to fulfill allocation requests. Therefore, it’s easy to degrade system responsiveness by allocating too much memory. For example, you can bring the computer down by pushing too much data into a scull device; the system starts crawling while it tries to swap out as much as possible in order to fulfill the kmalloc request. Since every resource is being sucked up by the growing device, the computer is soon rendered unusable; at that point, you can no longer even start a new process to try to deal with the problem. We don’t address this issue in scull, since it is just a sample module and not a real tool to put into a multiuser system. As a programmer, you must be careful nonetheless, because a module is privileged code and can open new security holes in the system (the most likely is a denial-of-service hole like the one just outlined).


***
<h3 id="8.3.1">8.3.1 一个使用整页的scull：scullp </h3>
***

In order to test page allocation for real, we have released the scullp module together with other sample code. It is a reduced scull, just like scullc introduced earlier.

Memory quanta allocated by scullp are whole pages or page sets: the scullp_order variable defaults to 0 but can be changed at either compile or load time.

The following lines show how it allocates memory:

    /* Here's the allocation of a single quantum */
    if (!dptr->data[s_pos]) {
        dptr->data[s_pos] = (void *)__get_free_pages(GFP_KERNEL, dptr->order);
        if (!dptr->data[s_pos])
            goto nomem;
        memset(dptr->data[s_pos], 0, PAGE_SIZE << dptr->order);
    }

The code to deallocate memory in scullp looks like this:

    /* This code frees a whole quantum-set */
    for (i = 0; i < qset; i++)
        if (dptr->data[i])
            free_pages((unsigned long)(dptr->data[i]), dptr->order);

At the user level, the perceived difference is primarily a speed improvement and better memory use, because there is no internal fragmentation of memory. We ran some tests copying 4 MB from scull0 to scull1 and then from scullp0 to scullp1 ; the results showed a slight improvement in kernel-space processor usage.

The performance improvement is not dramatic, because kmalloc is designed to be fast. The main advantage of page-level allocation isn’t actually speed, but rather more efficient memory usage. Allocating by pages wastes no memory, whereas using kmalloc wastes an unpredictable amount of memory because of allocation granularity.

But the biggest advantage of the __get_free_page functions is that the pages obtained are completely yours, and you could, in theory, assemble the pages into a linear area by appropriate tweaking of the page tables. For example, you can allow a user process to mmap memory areas obtained as single unrelated pages. We discuss this kind of operation in Chapter 15, where we show how scullp offers memory mapping, something that scull cannot offer.

***
<h3 id="8.3.2">8.3.2 alloc_pages接口 </h3>
***

For completeness, we introduce another interface for memory allocation, even though we will not be prepared to use it until after Chapter 15. For now, suffice it to say that struct page is an internal kernel structure that describes a page of memory. As we will see, there are many places in the kernel where it is necessary to work withpage structures; they are especially useful in any situation where you might be dealing with high memory, which does not have a constant address in kernel space.

The real core of the Linux page allocator is a function called alloc_pages_node:

    struct page *alloc_pages_node(int nid, unsigned int flags, unsigned int order);

This function also has two variants (which are simply macros); these are the versions that you will most likely use:

    struct page *alloc_pages(unsigned int flags, unsigned int order);
    struct page *alloc_page(unsigned int flags);

The core function, alloc_pages_node, takes three arguments. nid is the NUMA node ID* whose memory should be allocated, flags is the usual GFP_ allocation flags, and order is the size of the allocation. The return value is a pointer to the first of (possibly many) page structures describing the allocated memory, or, as usual, NULL on failure.

alloc_pages simplifies the situation by allocating the memory on the current NUMA node (it calls alloc_pages_node with the return value from numa_node_id as the nid parameter). And, of course, alloc_page omits the order parameter and allocates a single page.

To release pages allocated in this manner, you should use one of the following:

    void __free_page(struct page *page);
    void __free_pages(struct page *page, unsigned int order);
    void free_hot_page(struct page *page);
    void free_cold_page(struct page *page);

If you have specific knowledge of whether a single page’s contents are likely to be resident in the processor cache, you should communicate that to the kernel with free_hot_page (for cache-resident pages) or free_cold_page. This information helps the memory allocator optimize its use of memory across the system.

***
<h3 id="8.3.3">8.3.3 vmalloc and Friends </h3>
***

The next memory allocation function that we show you is vmalloc, which allocates a contiguous memory region in the virtual address space. Although the pages are not consecutive in physical memory (each page is retrieved with a separate call to alloc_page), the kernel sees them as a contiguous range of addresses. vmalloc returns 0 (the NULL address) if an error occurs, otherwise, it returns a pointer to a linear memory area of size at least size.

NUMA (nonuniform memory access) computers are multiprocessor systems where memory is “local” to specific groups of processors (“nodes”). Access to local memory is faster than access to nonlocal memory. On such systems, allocating memory on the correct node is important. Driver authors do not normally have to worry about NUMA issues, however.

We describe vmalloc here because it is one of the fundamental Linux memory allocation mechanisms. We should note, however, that use of vmalloc is discouraged in most situations. Memory obtained from vmalloc is slightly less efficient to work with, and, on some architectures, the amount of address space set aside for vmalloc is relatively small. Code that uses vmalloc is likely to get a chilly reception if submitted for inclusion in the kernel. If possible, you should work directly with individual pages rather than trying to smooth things over with vmalloc.

That said, let’s see how vmalloc works. The prototypes of the function and its relatives (ioremap, which is not strictly an allocation function, is discussed later in this section) are as follows:

    #include <linux/vmalloc.h>
    void *vmalloc(unsigned long size);
    void vfree(void * addr);
    void *ioremap(unsigned long offset, unsigned long size);
    void iounmap(void * addr);

It’s worth stressing that memory addresses returned by kmalloc and _get_free_pages are also virtual addresses. Their actual value is still massaged by the MMU (the memory management unit, usually part of the CPU) before it is used to address physical memory.* vmalloc is not different in how it uses the hardware, but rather in how the kernel performs the allocation task.

The (virtual) address range used by kmalloc and __get_free_pages features a one-toone mapping to physical memory, possibly shifted by a constant PAGE_OFFSET value; the functions don’t need to modify the page tables for that address range. The address range used by vmalloc and ioremap, on the other hand, is completely synthetic, and each allocation builds the (virtual) memory area by suitably setting up the page tables.

This difference can be perceived by comparing the pointers returned by the allocation functions. On some platforms (for example, the x86), addresses returned by vmalloc are just beyond the addresses that kmalloc uses. On other platforms (for example, MIPS, IA-64, and x86_64), they belong to a completely different address range. Addresses available for vmalloc are in the range from VMALLOC_START to VMALLOC_END. Both symbols are defined in <asm/pgtable.h>.

Addresses allocated by vmalloc can’t be used outside of the microprocessor, because they make sense only on top of the processor’s MMU. When a driver needs a real physical address (such as a DMA address, used by peripheral hardware to drive the system’s bus), you can’t easily use vmalloc. The right time to call vmalloc is when you are allocating memory for a large sequential buffer that exists only in software. It’s important to note that vmalloc has more overhead than __get_free_pages, because it must both retrieve the memory and build the page tables. Therefore, it doesn’t make sense to call vmalloc to allocate just one page.

Actually, some architectures define ranges of “virtual” addresses as reserved to address physical memory. When this happens, the Linux kernel takes advantage of the feature, and both the kernel and __get_free_pages addresses lie in one of those memory ranges. The difference is transparent to device drivers and other code that is not directly involved with the memory-management kernel subsystem.

An example of a function in the kernel that uses vmalloc is the create_module system call, which uses vmalloc to get space for the module being created. Code and data of the module are later copied to the allocated space using copy_from_user. In this way, the module appears to be loaded into contiguous memory. You can verify, by looking in /proc/kallsyms, that kernel symbols exported by modules lie in a different memory range from symbols exported by the kernel proper.

Memory allocated with vmalloc is released by vfree, in the same way that kfree releases memory allocated by kmalloc.

Like vmalloc, ioremap builds new page tables; unlike vmalloc, however, it doesn’t actually allocate any memory. The return value of ioremap is a special virtual address that can be used to access the specified physical address range; the virtual address obtained is eventually released by calling iounmap.

ioremap is most useful for mapping the (physical) address of a PCI buffer to (virtual) kernel space. For example, it can be used to access the frame buffer of a PCI video device; such buffers are usually mapped at high physical addresses, outside of the address range for which the kernel builds page tables at boot time. PCI issues are explained in more detail in Chapter 12.

It’s worth noting that for the sake of portability, you should not directly access addresses returned by ioremap as if they were pointers to memory. Rather, you should always use readb and the other I/O functions introduced in Chapter 9. This requirement applies because some platforms, such as the Alpha, are unable to directly map PCI memory regions to the processor address space because of differences between PCI specs and Alpha processors in how data is transferred.

Both ioremap and vmalloc are page oriented (they work by modifying the page tables); consequently, the relocated or allocated size is rounded up to the nearest page boundary. ioremap simulates an unaligned mapping by “rounding down” the address to be remapped and by returning an offset into the first remapped page. One minor drawback of vmalloc is that it can’t be used in atomic context because, internally, it uses kmalloc(GFP_KERNEL) to acquire storage for the page tables, and therefore could sleep. This shouldn’t be a problem—if the use of __get_free_page isn’t good enough for an interrupt handler, the software design needs some cleaning up.

***
<h3 id="8.3.4">8.3.4 一个使用虚拟地址的scull：scullv </h3>
***

Sample code using vmalloc is provided in the scullv module. Like scullp, this module is a stripped-down version of scull that uses a different allocation function to obtain space for the device to store data.

The module allocates memory 16 pages at a time. The allocation is done in large chunks to achieve better performance than scullp and to show something that takes too long with other allocation techniques to be feasible. Allocating more than one page with __get_free_pages is failure prone, and even when it succeeds, it can be slow. As we saw earlier, vmalloc is faster than other functions in allocating several pages, but somewhat slower when retrieving a single page, because of the overhead of page-table building. scullv is designed like scullp. order specifies the “order” of each allocation and defaults to 4. The only difference between scullv and scullp is in allocation management. These lines use vmalloc to obtain new memory:

    /* Allocate a quantum using virtual addresses */
    if (!dptr->data[s_pos]) {
        dptr->data[s_pos] = (void *)vmalloc(PAGE_SIZE << dptr->order);
        if (!dptr->data[s_pos])
            goto nomem;
        memset(dptr->data[s_pos], 0, PAGE_SIZE << dptr->order);
    }

and these lines release memory:
    /* Release the quantum-set */
    for (i = 0; i < qset; i++)
        if (dptr->data[i])
            vfree(dptr->data[i]);

If you compile both modules with debugging enabled, you can look at their data allocation by reading the files they create in /proc. This snapshot was taken on an x86_64 system:

    salma% cat /tmp/bigfile > /dev/scullp0; head -5 /proc/scullpmem
    Device 0: qset 500, order 0, sz 1535135
        item at 000001001847da58, qset at 000001001db4c000
            0:1001db56000
            1:1003d1c7000
    salma% cat /tmp/bigfile > /dev/scullv0; head -5 /proc/scullvmem
    Device 0: qset 500, order 4, sz 1535135
        item at 000001001847da58, qset at 0000010013dea000
            0:ffffff0001177000
            1:ffffff0001188000

The following output, instead, came from an x86 system:

    rudo% cat /tmp/bigfile > /dev/scullp0; head -5 /proc/scullpmem

    Device 0: qset 500, order 0, sz 1535135
        item at ccf80e00, qset at cf7b9800
            0:ccc58000
            1:cccdd000
    rudo% cat /tmp/bigfile > /dev/scullv0; head -5 /proc/scullvmem
    Device 0: qset 500, order 4, sz 1535135
        item at cfab4800, qset at cf8e4000
            0:d087a000
            1:d08d2000

The values show two different behaviors. On x86_64, physical addresses and virtual addresses are mapped to completely different address ranges (0x100 and 0xffffff00), while on x86 computers, vmalloc returns virtual addresses just above the mapping used for physical memory.

***
<h2 id="8.4">8.4 Per-CPU Variables </h2>
***

Per-CPU variables are an interesting 2.6 kernel feature. When you create a per-CPU variable, each processor on the system gets its own copy of that variable. This may seem like a strange thing to want to do, but it has its advantages. Access to per-CPU variables requires (almost) no locking, because each processor works with its own copy. Per-CPU variables can also remain in their respective processors’caches, which leads to significantly better performance for frequently updated quantities.

A good example of per-CPU variable use can be found in the networking subsystem. The kernel maintains no end of counters tracking how many of each type of packet was received; these counters can be updated thousands of times per second. Rather than deal with the caching and locking issues, the networking developers put the statistics counters into per-CPU variables. Updates are now lockless and fast. On the rare occasion that user space requests to see the values of the counters, it is a simple matter to add up each processor’s version and return the total. The declarations for per-CPU variables can be found in <linux/percpu.h>. To create a per-CPU variable at compile time, use this macro:

    DEFINE_PER_CPU(type, name);

If the variable (to be called name) is an array, include the dimension information with
the type. Thus, a per-CPU array of three integers would be created with:

    DEFINE_PER_CPU(int[3], my_percpu_array);

Per-CPU variables can be manipulated without explicit locking—almost. Remember
that the 2.6 kernel is preemptible; it would not do for a processor to be preempted in the middle of a critical section that modifies a per-CPU variable. It also would not be
good if your process were to be moved to another processor in the middle of a perCPU variable access. For this reason, you must explicitly use the get_cpu_var macro
to access the current processor’s copy of a given variable, and call put_cpu_var when
you are done. The call to get_cpu_var returns an lvalue for the current processor’s
version of the variable and disables preemption. Since an lvalue is returned, it can be
assigned to or operated on directly. For example, one counter in the networking code
is incremented with these two statements:

    get_cpu_var(sockets_in_use)++;
    put_cpu_var(sockets_in_use);

You can access another processor’s copy of the variable with:

    per_cpu(variable, int cpu_id);

If you write code that involves processors reaching into each other’s per-CPU variables, you, of course, have to implement a locking scheme that makes that access
safe.
Dynamically allocated per-CPU variables are also possible. These variables can be
allocated with:

    void *alloc_percpu(type);
    void *__alloc_percpu(size_t size, size_t align);

In most cases, alloc_percpu does the job; you can call __alloc_percpu in cases where
a particular alignment is required. In either case, a per-CPU variable can be returned
to the system with free_percpu. Access to a dynamically allocated per-CPU variable is
done via per_cpu_ptr:

    per_cpu_ptr(void *per_cpu_var, int cpu_id);

This macro returns a pointer to the version of per_cpu_var corresponding to the given
cpu_id. If you are simply reading another CPU’s version of the variable, you can dereference that pointer and be done with it. If, however, you are manipulating the current
processor’s version, you probably need to ensure that you cannot be moved out of
that processor first. If the entirety of your access to the per-CPU variable happens
with a spinlock held, all is well. Usually, however, you need to use get_cpu to block
preemption while working with the variable. Thus, code using dynamic per-CPU variables tends to look like this:

    int cpu;
    cpu = get_cpu( )
    ptr = per_cpu_ptr(per_cpu_var, cpu);
    /* work with ptr */
    put_cpu( );

When using compile-time per-CPU variables, the get_cpu_var and put_cpu_var macros take care of these details. Dynamic per-CPU variables require more explicit protection. Per-CPU variables can be exported to modules, but you must use a special version of the macros:

    EXPORT_PER_CPU_SYMBOL(per_cpu_var);
    EXPORT_PER_CPU_SYMBOL_GPL(per_cpu_var);

To access such a variable within a module, declare it with:

    DECLARE_PER_CPU(type, name);

The use of DECLARE_PER_CPU (instead of DEFINE_PER_CPU) tells the compiler
that an external reference is being made.

If you want to use per-CPU variables to create a simple integer counter, take a look
at the canned implementation in <linux/percpu_counter.h>. Finally, note that some
architectures have a limited amount of address space available for per-CPU variables. If you create per-CPU variables in your code, you should try to keep them
small.

***
<h2 id="8.5">8.5 Obtaining Large Buffers </h2>
***

As we have noted in previous sections, allocations of large, contiguous memory buffers are prone to failure. System memory fragments over time, and chances are that a
truly large region of memory will simply not be available. Since there are usually
ways of getting the job done without huge buffers, the kernel developers have not
put a high priority on making large allocations work. Before you try to obtain a large
memory area, you should really consider the alternatives. By far the best way of performing large I/O operations is through scatter/gather operations, which we discuss
in the section “Scatter-gather mappings” in Chapter 1.

***
<h3 id="8.5.1">8.5.1 Acquiring a Dedicated Buffer at Boot Time </h3>
***

If you really need a huge buffer of physically contiguous memory, the best approach
is often to allocate it by requesting memory at boot time. Allocation at boot time is
the only way to retrieve consecutive memory pages while bypassing the limits
imposed by __get_free_pages on the buffer size, both in terms of maximum allowed
size and limited choice of sizes. Allocating memory at boot time is a “dirty” technique, because it bypasses all memory management policies by reserving a private
memory pool. This technique is inelegant and inflexible, but it is also the least prone
to failure. Needless to say, a module can’t allocate memory at boot time; only drivers directly linked to the kernel can do that.

One noticeable problem with boot-time allocation is that it is not a feasible option
for the average user, since this mechanism is available only for code linked in the kernel image. A device driver using this kind of allocation can be installed or replaced
only by rebuilding the kernel and rebooting the computer.

When the kernel is booted, it gains access to all the physical memory available in the
system. It then initializes each of its subsystems by calling that subsystem’s initialization function, allowing initialization code to allocate a memory buffer for private use
by reducing the amount of RAM left for normal system operation.

Boot-time memory allocation is performed by calling one of these functions:

    #include <linux/bootmem.h>
    void *alloc_bootmem(unsigned long size);
    void *alloc_bootmem_low(unsigned long size);
    void *alloc_bootmem_pages(unsigned long size);
    void *alloc_bootmem_low_pages(unsigned long size);

The functions allocate either whole pages (if they end with _pages) or non-pagealigned memory areas. The allocated memory may be high memory unless one of the _low versions is used. If you are allocating this buffer for a device driver, you probably want to use it for DMA operations, and that is not always possible with high memory; thus, you probably want to use one of the _low variants.

It is rare to free memory allocated at boot time; you will almost certainly be unable to get it back later if you want it. There is an interface to free this memory, however:

    void free_bootmem(unsigned long addr, unsigned long size);

Note that partial pages freed in this manner are not returned to the system—but, if you are using this technique, you have probably allocated a fair number of whole pages to begin with.

If you must use boot-time allocation, you need to link your driver directly into the kernel. See the files in the kernel source under Documentation/kbuild for more information on how this should be done.