Although playing with scull and similar toys is a good introduction to the software interface of a Linux device driver, implementing a real device requires hardware. The driver is the abstraction layer between software concepts and hardware circuitry; as such, it needs to talkwith both of them. Up until now, we have examined the internals of software concepts; this chapter completes the picture by showing you how a driver can access I/O ports and I/O memory while being portable across Linux platforms.

This chapter continues in the tradition of staying as independent of specific hardware as possible. However, where specific examples are needed, we use simple digital I/O ports (such as the standard PC parallel port) to show how the I/O instructions work and normal frame-buffer video memory to show memory-mapped I/O.

We chose simple digital I/O because it is the easiest form of an input/output port. Also, the parallel port implements raw I/O and is available in most computers: data bits written to the device appear on the output pins, and voltage levels on the input pins are directly accessible by the processor. In practice, you have to connect LEDs or a printer to the port to actually see the results of a digital I/O operation, but the underlying hardware is extremely easy to use.


<h2 id="9.1">9.1 I/O 端口和 I/O 内存</h2>
***

每个外设都可以通过读写它的寄存器进行控制。大多数时候，外设具有多个寄存器，既可以通过内存连续地址也可以通过I/O连续地址空间访问它们。

At the hardware level, there is no conceptual difference between memory regions and I/O regions: both of them are accessed by asserting electrical signals on the address bus and control bus (i.e., the read and write signals)and by reading from or writing to the data bus.

While some CPU manufacturers implement a single address space in their chips, others decided that peripheral devices are different from memory and, therefore, deserve
a separate address space. Some processors (most notably the x86 family) have separate read and write electrical lines for I/O ports and special CPU instructions to
access ports.

Because peripheral devices are built to fit a peripheral bus, and the most popular I/O
buses are modeled on the personal computer, even processors that do not have a separate address space for I/O ports must fake reading and writing I/O ports when
accessing some peripheral devices, usually by means of external chipsets or extra circuitry in the CPU core. The latter solution is common within tiny processors meant
for embedded use.

For the same reason, Linux implements the concept of I/O ports on all computer
platforms it runs on, even on platforms where the CPU implements a single address
space. The implementation of port access sometimes depends on the specific make
and model of the host computer (because different models use different chipsets to
map bus transactions into memory address space).

Even if the peripheral bus has a separate address space for I/O ports, not all devices
map their registers to I/O ports. While use of I/O ports is common for ISA peripheral boards, most PCI devices map registers into a memory address region. This I/O
memory approach is generally preferred, because it doesn’t require the use of specialpurpose processor instructions; CPU cores access memory much more efficiently,
and the compiler has much more freedom in register allocation and addressing-mode
selection when accessing memory.

<h3 id="9.1.1">9.1.1 I/O寄存器和常规内存 </h3>
***

Despite the strong similarity between hardware registers and memory, a programmer accessing I/O registers must be careful to avoid being tricked by CPU (or compiler) optimizations that can modify the expected I/O behavior.

The main difference between I/O registers and RAM is that I/O operations have side effects, while memory operations have none: the only effect of a memory write is storing a value to a location, and a memory read returns the last value written there. Because memory access speed is so critical to CPU performance, the no-side-effects case has been optimized in several ways: values are cached and read/write instructions are reordered.

The compiler can cache data values into CPU registers without writing them to
memory, and even if it stores them, both write and read operations can operate on
cache memory without ever reaching physical RAM. Reordering can also happen
both at the compiler level and at the hardware level: often a sequence of instructions
can be executed more quickly if it is run in an order different from that which
appears in the program text, for example, to prevent interlocks in the RISC pipeline.
On CISC processors, operations that take a significant amount of time can be executed concurrently with other, quicker ones.

These optimizations are transparent and benign when applied to conventional memory (at least on uniprocessor systems), but they can be fatal to correct I/O operations, because they interfere with those “side effects” that are the main reason why a
driver accesses I/O registers. The processor cannot anticipate a situation in which
some other process (running on a separate processor, or something happening inside
an I/O controller) depends on the order of memory access. The compiler or the CPU
may just try to outsmart you and reorder the operations you request; the result can
be strange errors that are very difficult to debug. Therefore, a driver must ensure that
no caching is performed and no read or write reordering takes place when accessing
registers.

The problem with hardware caching is the easiest to face: the underlying hardware is
already configured (either automatically or by Linux initialization code) to disable
any hardware cache when accessing I/O regions (whether they are memory or port
regions).

The solution to compiler optimization and hardware reordering is to place a memory
barrier between operations that must be visible to the hardware (or to another processor) in a particular order. Linux provides four macros to cover all possible ordering needs:

#include <linux/kernel.h>
void barrier(void)

    This function tells the compiler to insert a memory barrier but has no effect onesthe hardware. Compiled code stores to memory all values that are currently modified and resident in CPU registers, and rereads them later when they are needed. A call to barrier prevents compiler optimizations across the barrier but leaves the hardware free to do its own reordering。

#include <asm/system.h>
void rmb(void);
void read_barrier_depends(void);
void wmb(void);
void mb(void);

    These functions insert hardware memory barriers in the compiled instruction flow; their actual instantiation is platform dependent. An rmb (read memory barrier) guarantees that any reads appearing before the barrier are completed prior to the execution of any subsequent read. wmb guarantees ordering in write operations, and the mb instruction guarantees both. Each of these functions is a superset of barrier.

    read_barrier_depends is a special, weaker form of read barrier. Whereas rmb prevents the reordering of all reads across the barrier, read_barrier_depends blocks only the reordering of reads that depend on data from other reads. The distinction is subtle, and it does not exist on all architectures. Unless you understand exactly what is going on, and you have a reason to believe that a full read barrier is exacting an excessive performance cost, you should probably stickto using rmb.

void smp_rmb(void);
void smp_read_barrier_depends(void);
void smp_wmb(void);
void smp_mb(void);

    These versions of the barrier macros insert hardware barriers only when the kernel is compiled for SMP systems; otherwise, they all expand to a simple barrier call.

A typical usage of memory barriers in a device driver may have this sort of form:

    writel(dev->registers.addr, io_destination_address);
    writel(dev->registers.size, io_size);
    writel(dev->registers.operation, DEV_READ);
    wmb( );
    writel(dev->registers.control, DEV_GO);

<h2 id="9.2">9.2 使用 I/O 端口</h2>
***

<h3 id="9.2.1">9.2.1 I/O Port Allocation </h3>
***

<h3 id="9.2.2">9.2.2 Manipulating I/O ports </h3>
***

<h3 id="9.2.3">9.2.3 I/O Port Access from User Space </h3>
***

<h3 id="9.2.4">9.2.4 String Operations </h3>
***

<h3 id="9.2.5">9.2.5 Pausing I/O </h3>
***

<h3 id="9.2.6">9.2.6 Platform Dependencies </h3>
***

<h2 id="9.3">9.3 An I/O Port Example</h2>
***

<h3 id="9.3.1">9.3.1 An Overview of the Parallel Port </h3>
***

<h3 id="9.3.2">9.3.2 A Sample Driver </h3>
***

<h2 id="9.4">9.4 Using I/O Memory</h2>
***

<h3 id="9.4.1">9.4.1 I/O Memory Allocation and Mapping </h3>
***

<h3 id="9.4.2">9.4.2 Accessing I/O Memory </h3>
***

<h3 id="9.4.3">9.4.3 Ports as I/O Memory </h3>
***

<h3 id="9.4.4">9.4.4 Reusing short for I/O Memory </h3>
***

<h3 id="9.4.5">9.4.5 ISA Memory Below 1 MB </h3>
***

<h3 id="9.4.6">9.4.6 isa_readb and Friends </h3>
***