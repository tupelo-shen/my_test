* [9.1 I/O 端口和 I/O 内存](#9.1 )
    - [9.1.1 I/O寄存器和常规内存](#9.1.1 )
* [9.2 使用 I/O 端口](#9.2 )
    * [9.2.1 I/O Port Allocation](#9.2.1 )
    * [9.2.2 Manipulating I/O ports ](#9.2.2 )
    * [9.2.3 I/O Port Access from User Space ](#9.2.3 )
    * [9.2.4 String Operations ](#9.2.4 )
    * [9.2.5 Pausing I/O ](#9.2.5 )
    * [9.2.6 Platform Dependencies ](#9.2.6 )
* [9.3 An I/O Port Example](#9.3 )
    * [9.3.1 An Overview of the Parallel Port ](#9.3.1 )
    * [9.3.2 A Sample Driver ](#9.3.2 )
* [9.4 Using I/O Memory](#9.4 )
    * [9.4.1 I/O Memory Allocation and Mapping ](#9.4.1 )
    * [9.4.2 Accessing I/O Memory ](#9.4.2 )
    * [9.4.3 Ports as I/O Memory ](#9.4.3 )
    * [9.4.4 Reusing short for I/O Memory ](#9.4.4 )
    * [9.4.5 ISA Memory Below 1 MB ](#9.4.5 )
    * [9.4.6 isa_readb and Friends ](#9.4.6 )

***

scull及其变体是介绍linux设备驱动相关的软件接口和概念等的良好载体，因为实现真实的硬件设备是需要硬件的。驱动是软件和硬件电路的中间抽象层；正因如此，驱动既需要与软件通信，又要与硬件通信。到目前为止，我们一直在研究软件概念，所以，接下来我们展示驱动是如何访问I/O端口和I/O内存，并且是可移植的。

本章尽量不依赖于具体的硬件设备。但是，如果某个示例需要，我们将使用简单的数字I/O端口（比如标准的PC并口）去展示I/O指令的工作方式， 使用普通的帧缓存显卡内存展示内存映射的I/O。

简单的数字I/O是最简单的输入输出端口的实现形式之一。在大多数计算机上，都有并口，是原始的I/O：写入设备的数据就会出现在输出管脚上，输入管脚的电压可以直接被处理器访问。实际使用中，你可能需要连接LED或者打印机才能看到数字I/O操作的效果，但是这些底层硬件都很简单易用。

<h2 id="9.1">9.1 I/O 端口和 I/O 内存</h2>
***

每个外设都可以通过读写它的寄存器进行控制。大多数时候，外设具有多个寄存器，既可以通过内存连续地址也可以通过I/O连续地址空间访问它们。

从硬件层面来说，内存区域和I/O区域没有概念上的区别：它们都可以通过地址总线和控制总线进行访问（例如，读写信号），读写数据通过数据总线。

有些CPU实现单一的地址空间，而另外一些就是外设和内存使用不同的地址空间。有些处理器（尤其是x86体系）拥有分别独立的读、写信号线和特殊的CPU指令访问端口。

Because peripheral devices are built to fit a peripheral bus, and the most popular I/O buses are modeled on the personal computer, even processors that do not have a separate address space for I/O ports must fake reading and writing I/O ports when accessing some peripheral devices, usually by means of external chipsets or extra circuitry in the CPU core. The latter solution is common within tiny processors meant for embedded use.


For the same reason, Linux implements the concept of I/O ports on all computer platforms it runs on, even on platforms where the CPU implements a single address space. The implementation of port access sometimes depends on the specific make and model of the host computer (because different models use different chipsets to map bus transactions into memory address space).

Even if the peripheral bus has a separate address space for I/O ports, not all devices map their registers to I/O ports. While use of I/O ports is common for ISA peripheral boards, most PCI devices map registers into a memory address region. This I/O memory approach is generally preferred, because it doesn’t require the use of special purpose processor instructions; CPU cores access memory much more efficiently, and the compiler has much more freedom in register allocation and addressing-mode selection when accessing memory.

<h3 id="9.1.1">9.1.1 I/O寄存器和常规内存 </h3>
***

Despite the strong similarity between hardware registers and memory, a programmer accessing I/O registers must be careful to avoid being tricked by CPU (or compiler) optimizations that can modify the expected I/O behavior.

The main difference between I/O registers and RAM is that I/O operations have side effects, while memory operations have none: the only effect of a memory write is storing a value to a location, and a memory read returns the last value written there. Because memory access speed is so critical to CPU performance, the no-side-effects case has been optimized in several ways: values are cached and read/write instructions are reordered.

The compiler can cache data values into CPU registers without writing them to memory, and even if it stores them, both write and read operations can operate on cache memory without ever reaching physical RAM. Reordering can also happen both at the compiler level and at the hardware level: often a sequence of instructions can be executed more quickly if it is run in an order different from that which appears in the program text, for example, to prevent interlocks in the RISC pipeline. On CISC processors, operations that take a significant amount of time can be executed concurrently with other, quicker ones.

These optimizations are transparent and benign when applied to conventional memory (at least on uniprocessor systems), but they can be fatal to correct I/O operations, because they interfere with those “side effects” that are the main reason why a driver accesses I/O registers. The processor cannot anticipate a situation in which some other process (running on a separate processor, or something happening inside an I/O controller) depends on the order of memory access. The compiler or the CPU may just try to outsmart you and reorder the operations you request; the result can be strange errors that are very difficult to debug. Therefore, a driver must ensure that no caching is performed and no read or write reordering takes place when accessing registers.

The problem with hardware caching is the easiest to face: the underlying hardware is already configured (either automatically or by Linux initialization code) to disable any hardware cache when accessing I/O regions (whether they are memory or port regions).

The solution to compiler optimization and hardware reordering is to place a memory barrier between operations that must be visible to the hardware (or to another processor) in a particular order. Linux provides four macros to cover all possible ordering needs:

    #include <linux/kernel.h>
    void barrier(void)

This function tells the compiler to insert a memory barrier but has no effect on the hardware. Compiled code stores to memory all values that are currently modified and resident in CPU registers, and rereads them later when they are needed. A call to barrier prevents compiler optimizations across the barrier but leaves the hardware free to do its own reordering。

    #include <asm/system.h>
    void rmb(void);
    void read_barrier_depends(void);
    void wmb(void);
    void mb(void);

These functions insert hardware memory barriers in the compiled instruction flow; their actual instantiation is platform dependent. An rmb (read memory barrier) guarantees that any reads appearing before the barrier are completed prior to the execution of any subsequent read. wmb guarantees ordering in write operations, and the mb instruction guarantees both. Each of these functions is a superset of barrier.

read_barrier_depends is a special, weaker form of read barrier. Whereas rmb prevents the reordering of all reads across the barrier, read_barrier_depends blocks only the reordering of reads that depend on data from other reads. The distinction is subtle, and it does not exist on all architectures. Unless you understand exactly what is going on, and you have a reason to believe that a full read barrier is exacting an excessive performance cost, you should probably stickto using rmb.

    void smp_rmb(void);
    void smp_read_barrier_depends(void);
    void smp_wmb(void);
    void smp_mb(void);

These versions of the barrier macros insert hardware barriers only when the kernel is compiled for SMP systems; otherwise, they all expand to a simple barrier call.

A typical usage of memory barriers in a device driver may have this sort of form:

    writel(dev->registers.addr, io_destination_address);
    writel(dev->registers.size, io_size);
    writel(dev->registers.operation, DEV_READ);
    wmb( );
    writel(dev->registers.control, DEV_GO);

<h2 id="9.2">9.2 使用 I/O 端口</h2>
***

<h3 id="9.2.1">9.2.1 I/O Port Allocation </h3>
***

<h3 id="9.2.2">9.2.2 Manipulating I/O ports </h3>
***

<h3 id="9.2.3">9.2.3 I/O Port Access from User Space </h3>
***

<h3 id="9.2.4">9.2.4 String Operations </h3>
***

<h3 id="9.2.5">9.2.5 Pausing I/O </h3>
***

<h3 id="9.2.6">9.2.6 Platform Dependencies </h3>
***

<h2 id="9.3">9.3 An I/O Port Example</h2>
***

<h3 id="9.3.1">9.3.1 An Overview of the Parallel Port </h3>
***

<h3 id="9.3.2">9.3.2 A Sample Driver </h3>
***

<h2 id="9.4">9.4 Using I/O Memory</h2>
***

<h3 id="9.4.1">9.4.1 I/O Memory Allocation and Mapping </h3>
***

<h3 id="9.4.2">9.4.2 Accessing I/O Memory </h3>
***

<h3 id="9.4.3">9.4.3 Ports as I/O Memory </h3>
***

<h3 id="9.4.4">9.4.4 Reusing short for I/O Memory </h3>
***

<h3 id="9.4.5">9.4.5 ISA Memory Below 1 MB </h3>
***

<h3 id="9.4.6">9.4.6 isa_readb and Friends </h3>
***