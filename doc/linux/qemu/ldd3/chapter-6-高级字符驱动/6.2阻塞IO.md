<h1 id="0">0 目录</h1>
* [6.2 阻塞I/O](#6.2)
    - [6.2.1 休眠的介绍](#6.2.1)
    - [6.2.2 简单休眠](#6.2.2)
    - [6.2.3 阻塞和非阻塞操作](#6.2.3)
    - [6.2.4 一个阻塞I/O例子](#6.2.4)
    - [6.2.5 高级休眠](#6.2.5)
        + [6.2.5.1 进程如何休眠](#6.2.5.1)
        + [6.2.5.2 手动休眠](#6.2.5.2)
        + [6.2.5.3 独占等待](#6.2.5.3)
        + [6.2.5.4 唤醒的细节](#6.2.5.4)
        + [6.2.5.5 旧接口](#6.2.5.5)
    - [6.2.6 测试scullpipe驱动](#6.2.6)

<h2 id="6.2">6.2 阻塞I/O</h2>

在第3章中，我们已经编写了scull设备read和write方法，但是我们忽略了一个重要的问题，如果当我们read时，scull设备还没有数据可用怎么办？当我们write时，scull设备还没准备好接收数据怎么办？在这些情况下，驱动程序默认应该阻塞进程，让其休眠直至发出的请求可以被响应。

本章展示了怎样使一个进程休眠和何时唤醒。首先，先来了解一些新概念。

<h3 id="6.2.1">6.2.1 休眠的介绍</h3>

什么是休眠？休眠就是进程被标记为特殊状态并从调度器管理的运行队列中删除。除非外界修改这种状态，否则进程就不会被任何CPU调度。

在linux设备驱动中使进程休眠是一件很简单的事情。但是，要想使代码安全的休眠，必须注意两条规则：

* 第一条规则：原子操作的上下文中不能休眠。

    在第5章我们已经了解了，原子操作是阻止并发访问的一种手段。那么也就意味着你的驱动程序持有自旋锁，seqlock，或RCU lock时，不能进行休眠。当然了，如果中断被失能，也不能进行休眠。拥有信号量可以休眠，但是必须小心。如果拥有信号量的代码休眠，那么其它等待信号量的代码也会休眠。因此，拥有信号量的代码休眠时间必须短，且你应该确信，不会阻塞那些最终会唤醒你的那些进程。

* 另一条规则就是：唤醒后不能假设系统的状态，必须检查确保等待的条件是真的。

    因为当唤醒时，肯定不会知道已经失去CPU控制权多长时间；同时，在这段时间内发生了什么。另外，你通常也不会知道是否有另一个进程也在等待同一事件而在休眠；那个进程可能在你的进程之前被唤醒同时掠夺你的进程可能在等待的资源。

当唤醒事件发生时，通知wait队列（等待某一事件的进程列表）。

在linux中，使用wait_queue_head_t结构体来管理wait队列，其定义位于头文件`<linux/wait.h>`中。其初始化的形式如下：

    DECLARE_WAIT_QUEUE_HEAD(name);

或动态初始化，

    wait_queue_head_t my_queue;
    init_waitqueue_head(&my_queue);

<h3 id="6.2.2">6.2.2 简单休眠</h3>

Linux内核中，使进程进入休眠的最简单方法，如下：

    wait_event(queue, condition)
    wait_event_interruptible(queue, condition)
    wait_event_timeout(queue, condition, timeout)
    wait_event_interruptible_timeout(queue, condition, timeout)

* queue就是wait_queue_head_t类型的队列，注意是值传递的方式。
* condition就是休眠前后通过宏计算的任意bool表达式；除非condition称为true，否则一直休眠。值得注意的是，condition是次数的任意数字，所以不能有任何副作用。

如果使用wait_event，进程将会进入不可中断的休眠中，这应该不是你想要的结果。优先选择的替代方案就是，wait_event_interruptible，它可被信号中断，返回整数值。非0，说明休眠被某一类信号中断，驱动程序就应该返回-ERESTARTSYS。wait_event_timeout和wait_event_interruptible_timeout定周期等待，如果时间到，无论condition是否满足条件，都返回0。

唤醒休眠的进程的语法是：

    void wake_up(wait_queue_head_t *queue);
    void wake_up_interruptible(wait_queue_head_t *queue);

wake_up唤醒给定队列中所有正在等待进程（当然了实际情况非常复杂，我们稍后会看到）。wake_up_interruptible只唤醒可中断休眠。通常情况下，与等待被唤醒的语句配套使用。比如wake_up和wait_event配合使用。

我们下面看一个关于休眠和唤醒的简单例子。模块我们称之为sleepy。表现行为：任何试图读取设备的进程都会进入休眠；无论什么时候有一个进程写设备，都会唤醒所有的进程。读写方法如下，所示：

    static DECLARE_WAIT_QUEUE_HEAD(wq);
    static int flag = 0;
    ssize_t sleepy_read (struct file *filp, char __user *buf, size_t count, loff_t *pos)
    {
        printk(KERN_DEBUG "process %i (%s) going to sleep\n", current->pid,
            current->comm);
        wait_event_interruptible(wq, flag != 0);
        flag = 0;
        printk(KERN_DEBUG "awoken %i (%s)\n", current->pid, current->comm);
        return 0; /* EOF */
    }
    ssize_t sleepy_write (struct file *filp, const char __user *buf, size_t count,
        loff_t *pos)
    {
        printk(KERN_DEBUG "process %i (%s) awakening the readers...\n",
        current->pid, current->comm);
        flag = 1;
        wake_up_interruptible(&wq);
        return count; /* succeed, to avoid retrial */
    }

Note the use of the flag variable in this example. Since `wait_event_interruptible` checks for a condition that must become true, we use flag to create that condition.

It is interesting to consider what happens if two processes are waiting when `sleepy_write` is called. Since `sleepy_read` resets `flag` to 0 once it wakes up, you might think that the second process to wake up would immediately go back to sleep. On a single-processor system, that is almost always what happens. But it is important to understand why you cannot count on that behavior. The `wake_up_interruptible` call will cause both sleeping processes to wake up. It is entirely possible that they will both note that flag is nonzero before either has the opportunity to reset it. For this trivial module, this race condition is unimportant. In a real driver, this kind of race can create rare crashes that are difficult to diagnose. If correct operation required that exactly one process see the nonzero value, it would have to be tested in an atomic manner. We will see how a real driver handles such situations shortly. But first we have to cover one other topic.

<h3 id="6.2.3">6.2.3 阻塞和非阻塞操作</h3>

One last point we need to touch on before we look at the implementation of full-featured `read` and `write` methods is deciding when to put a process to sleep. There are times when implementing proper Unix semantics requires that an operation not block, even if it cannot be completely carried out.

There are also times when the calling process informs you that it does not want to block, whether or not its I/O can make any progress at all. Explicitly nonblocking I/O is indicated by the `O_NONBLOCK` flag in `filp->f_flags`. The flag is defined in `<linux/fcntl.h>`, which is automatically included by `<linux/fs.h>`. The flag gets its name from “open-nonblock,” because it can be specified at open time (and originally could be specified only there). If you browse the source code, you find some references to an `O_NDELAY` flag; this is an alternate name for `O_NONBLOCK`, accepted for compatibility with System V code. The flag is cleared by default, because the normal behavior of a process waiting for data is just to sleep. In the case of a blocking operation, which is the default, the following behavior should be implemented in order to adhere to the standard semantics:

* If a process calls `read` but no data is (yet) available, the process must block. The process is awakened as soon as some data arrives, and that data is returned to the caller, even if there is less than the amount requested in the `count` argument to the method.

* If a process calls `write` and there is no space in the buffer, the process must block, and it must be on a different wait queue from the one used for reading. When some data has been written to the hardware device, and space becomes free in the output buffer, the process is awakened and the `write` call succeeds, although the data may be only partially written if there isn’t room in the buffer for the `count` bytes that were requested.

Both these statements assume that there are both input and output buffers; in practice, almost every device driver has them. The input buffer is required to avoid losing data that arrives when nobody is reading. In contrast, data can’t be lost on write, because if the system call doesn’t accept data bytes, they remain in the user-space buffer. Even so, the output buffer is almost always useful for squeezing more performance out of the hardware.

The performance gain of implementing an output buffer in the driver results from the reduced number of context switches and user-level/kernel-level transitions. Without an output buffer (assuming a slow device), only one or a few characters are accepted by each system call, and while one process sleeps in write, another process runs (that’s one context switch). When the first process is awakened, it resumes (another context switch), write returns (kernel/user transition), and the process reiterates the system call to write more data (user/kernel transition); the call blocks and the loop continues. The addition of an output buffer allows the driver to accept larger chunks of data with each write call, with a corresponding increase in performance. If that buffer is big enough, the write call succeeds on the first attempt—the buffered data will be pushed out to the device later—without control needing to go back to user space for a second or third write call. The choice of a suitable size for the output buffer is clearly device-specific.

We don’t use an input buffer in `scull`, because data is already available when read is issued. Similarly, no output buffer is used, because data is simply copied to the memory area associated with the device. Essentially, the device is a buffer, so the implementation of additional buffers would be superfluous. We’ll see the use of buffers in Chapter 10.

The behavior of `read` and `write` is different if `O_NONBLOCK` is specified. In this case, the calls simply return -EAGAIN (“try it again”) if a process calls `read` when no data is available or if it calls `write` when there’s no space in the buffer.

As you might expect, nonblocking operations return immediately, allowing the application to poll for data. Applications must be careful when using the `stdio` functions while dealing with nonblocking files, because they can easily mistake a nonblocking return for EOF. They always have to check errno.

Naturally, O_NONBLOCK is meaningful in the `open` method also. This happens when the call can actually block for a long time; for example, when opening (for read access) a FIFO that has no writers (yet), or accessing a disk file with a pending lock. Usually, opening a device either succeeds or fails, without the need to wait for external events. Sometimes, however, opening the device requires a long initialization, and you may choose to support O_NONBLOCK in your `open` method by returning immediately with -EAGAIN if the flag is set, after starting the device initialization process. The driver may also implement a blocking open to support access policies in a way similar to file locks. We’ll see one such implementation in the section “Blocking open as an Alternative to EBUSY” later in this chapter.

Some drivers may also implement special semantics for O_NONBLOCK; for example, an open of a tape device usually blocks until a tape has been inserted. If the tape drive is opened with O_NONBLOCK, the open succeeds immediately regardless of whether the media is present or not.

Only the read, write, and open file operations are affected by the nonblocking flag.

> <font color="blue">总结：</font>
>
> <font color="blue">1. 使用阻塞和非阻塞的一些基本原则，read和write是不同的；</font>
>
> <font color="blue">2. 阐述了在示例中为什么没有使用buffer，因为本身这个模拟的设备就是基于内存的一个buffer；</font>
>
> <font color="blue">3. 阐述了write和read时，O_NONBLOCK的区别和使用的场合；</font>

<h3 id="6.2.4">6.2.4 一个阻塞I/O例子</h3>

Finally, we get to an example of a real driver method that implements blocking I/O.
This example is taken from the scullpipe driver; it is a special form of scull that implements
a pipe-like device.

Within a driver, a process blocked in a read call is awakened when data arrives; usually
the hardware issues an interrupt to signal such an event, and the driver awakens
waiting processes as part of handling the interrupt. The scullpipe driver works differently,
so that it can be run without requiring any particular hardware or an interrupt
handler. We chose to use another process to generate the data and wake the reading
process; similarly, reading processes are used to wake writer processes that are waiting
for buffer space to become available.

The device driver uses a device structure that contains two wait queues and a buffer.
The size of the buffer is configurable in the usual ways (at compile time, load time, or
runtime).

    struct scull_pipe {
        wait_queue_head_t       inq, outq;          /* read 和 write队列 */
        char                    *buffer, *end;      /* 缓冲区的头尾 */
        int                     buffersize;         /* 缓冲区大小 */
        char                    *rp, *wp;           /* 读写位置指针 */
        int                     nreaders, nwriters; /* r/w操作的数量 */
        struct fasync_struct    *async_queue;       /* 异步读取者 */
        struct semaphore        sem;                /* 互斥信号量 */
        struct cdev             cdev;               /* 字符设备结构 */
    };

read方法负责管理阻塞型和非阻塞型输入，如下所示：

    static ssize_t scull_p_read(struct file *filp, char __user *buf, size_t count,
        loff_t *f_pos)
    {
        struct scull_pipe *dev = filp->private_data;

        if(down_interruptible(&dev->sem))
            return -ERESTARTSYS;

        while (dev->rp == dev->wp) {                                        /* 什么也没读到 */
            up(&dev->sem);                                                  /* 释放锁 */
            if (filp->f_flags & O_NONBLOCK)
                return -EAGAIN;
            PDEBUG("\"%s\" reading: going to sleep\n", current->comm);
            if (wait_event_interruptible(dev->inq, (dev->rp != dev->wp)))
                return -ERESTARTSYS;                                        /* 信号: 通知fs层进行处理 */
            /* 否则循环，但是首先获取锁 */
            if (down_interruptible(&dev->sem))
                return -ERESTARTSYS;
        }

        /* 数据已就绪，返回 */
        if (dev->wp > dev->rp)
            count = min(count, (size_t)(dev->wp - dev->rp));
        else /* 写入指针回卷，返回数据直到dev->end */
            count = min(count, (size_t)(dev->end - dev->rp));
        if (copy_to_user(buf, dev->rp, count)) {
            up(&dev->sem);
            return -EFAULT;
        }

        dev->rp += count;
        if (dev->rp == dev->end)
            dev->rp = dev->buffer; /* 回卷 */
        up(&dev->sem);
        /* 最后，唤醒所有写入者并返回 */
        wake_up_interruptible(&dev->outq);
        PDEBUG("\"%s\" did read %li bytes\n",current->comm, (long)count);
        return count;
    }

PDEBUG行代码用于调试。

分析`scull_p_read`是如何处理等待数据的。while循环在拥有设备信号量时，测试缓冲区。如果其中有数据，则可以立即将数据返回给用户而不需要休眠，这样，循环内的内容就被跳过了。如果缓冲区为空，那么休眠。但在休眠之前必须释放设备信号量。因为如果在拥有信号量的时候休眠，那么任何写入者都没有机会来唤醒。在释放信号量之后，快速检查用户请求的是否是非阻塞I/O，如果是，则返回，否则调用`wake_up_interruptible`。

<h3 id="6.2.5">6.2.5 高级休眠</h3>

到目前为止，对于大多数驱动程序来说，已经介绍的函数足够满足休眠操作的需要了。但是，有些场合，可能需要对linux等待队列工作机理进行深入的了解。复杂的锁或性能方面的要求，都有可能强迫驱动程序使用更底层的函数去实现休眠。本节中，我们将会看到更底层的函数是如何影响进程休眠的。



<h4 id="6.2.5.1">6.2.5.1 进程如何休眠</h4>
<h4 id="6.2.5.2">6.2.5.2 手动休眠</h4>
<h4 id="6.2.5.3">6.2.5.3 独占等待</h4>
<h4 id="6.2.5.4">6.2.5.4 唤醒的细节</h4>
<h4 id="6.2.5.5">6.2.5.5 旧接口</h4>

<h3 id="6.2.6">6.2.6 测试scullpipe驱动</h3>